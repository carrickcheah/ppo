# Configuration for toy scheduling environment

environment:
  name: "ToySchedulingEnv"
  n_machines: 2
  n_jobs: 5
  max_episode_steps: 50
  min_job_time: 1
  max_job_time: 5
  seed: 42

training:
  # Total timesteps to train
  total_timesteps: 10000
  
  # Evaluation settings
  eval_freq: 5000
  n_eval_episodes: 10
  
  # Model saving
  save_freq: 10000
  save_path: "./models/toy_scheduler/"
  
  # Logging
  log_path: "./logs/toy_scheduler/"
  tensorboard_log: "./tensorboard/toy_scheduler/"

ppo:
  # Basic PPO hyperparameters
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null  # No value function clipping
  
  # Entropy regularization
  ent_coef: 0.01
  
  # Value function coefficient
  vf_coef: 0.5
  
  # Gradient clipping
  max_grad_norm: 0.5
  
  # Use generalized advantage estimation
  use_sde: false  # Don't use state dependent exploration
  sde_sample_freq: -1
  
  # Network architecture
  policy_kwargs:
    net_arch:
      - 64
      - 64
    activation_fn: "tanh"
    
  # Normalization
  normalize_advantage: true
  
  # Device
  device: "auto"  # Use GPU if available

# Random seeds for reproducibility
seeds:
  environment: 42
  training: 42
  eval: 1337

# Experiment tracking
experiment:
  name: "toy_scheduler_baseline"
  description: "Initial PPO training on toy scheduling environment"
  tags:
    - "toy_env"
    - "baseline"
    - "2_machines"
    - "5_jobs"