# Deep Reinforcement Learning Scheduling System

## Project Status: Phase 3 Training In Progress ğŸš§

### Pure DRL System (/app_2) - Current Status
- **Phase 1 & 2**: âœ… COMPLETE - Environment and PPO model implemented
- **Phase 3**: ğŸš§ IN PROGRESS - Curriculum learning (6/16 stages complete)
- **Current Issue**: Small rush stage achieving 0% utilization - model avoiding scheduling
- **Data**: 295 real production jobs from 88 families, 145 machines

## Latest Updates (July 24, 2025)

### Phase 3 - Curriculum Learning Progress
- **Training Status**:
  - Stages 1-6: âœ… COMPLETE (toy environments and small balanced)
  - Stage 7 (Small Rush): âš ï¸ 0% utilization issue discovered
  - Stages 8-16: PENDING
- **Small Rush Problem Analysis**:
  - Model learns to avoid scheduling entirely (0% utilization)
  - Root cause: Reward structure penalizes late jobs more than idle time
  - Rush orders have tight deadlines, mostly guaranteed to be late
  - Required fix: Add completion bonus, adjust penalties, increase exploration

### Phase 2 - PPO Model Implementation âœ…
- **Architecture**: Transformer with attention for variable job counts
- **Testing**: 100% success rate (14/14 tests passing)
- **Data**: Using real production snapshot with 295 jobs, 145 machines

### Phase 1.5 & 1.6 - Critical Data Fixes âœ…
- **Multi-machine jobs**: Require ALL specified machines simultaneously
- **Processing time**: Capacity-based formula implemented
- **Working hours**: Removed from training (deployment only)

## System Architecture

### Pure DRL System - Game-Based Learning
```
MariaDB â†’ Snapshot â†’ Game Environment â†’ PPO Player â†’ Schedule
   â†“         â†“             â†“                â†“           â†“
[Real]   [295 jobs]   [Rules Engine]   [Transformer]  [Output]
[Data]   [145 mach]   [No hardcoding]  [Attention]    [Optimal]
```

### Key Components
1. **Data Layer**
   - Production snapshot with real data
   - Multi-machine job parsing
   - Capacity-based processing times

2. **Environment**
   - Hard constraints as physics
   - Soft constraints as rewards
   - No working hours in training

3. **PPO Model**
   - Transformer for variable sizes
   - Action masking for validity
   - Curriculum learning stages

## Data Summary

### Production Snapshot
- **Jobs**: 295 tasks from 88 families
- **Machines**: 145 active machines
- **Multi-Machine Jobs**: 5 jobs requiring 2-5 machines
- **Processing Times**: 0.5 - 100+ hours
- **Deadlines**: 0 - 30 days remaining

### Snapshot Benefits
- 500x faster than database queries
- Consistent data for training
- Offline capability
- Reproducible experiments

## Quick Start

### Running Tests
```bash
cd /Users/carrickcheah/Project/ppo/app_2
uv run python phase2/test_result/comprehensive_test.py
```

### Starting Training (Phase 3)
```bash
cd /Users/carrickcheah/Project/ppo/app_2
uv run python phase2/train.py --config configs/training.yaml
```

## Project Structure
```
/app_2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/           # Database & snapshot loading âœ…
â”‚   â”œâ”€â”€ environment/    # Game rules and physics âœ…
â”‚   â””â”€â”€ deployment/     # API server (Phase 4)
â”œâ”€â”€ phase2/
â”‚   â”œâ”€â”€ state_encoder.py         # Variable input handling âœ…
â”‚   â”œâ”€â”€ transformer_policy.py    # Attention mechanism âœ…
â”‚   â”œâ”€â”€ action_masking.py        # Valid moves only âœ…
â”‚   â”œâ”€â”€ ppo_scheduler.py         # Core PPO algorithm âœ…
â”‚   â”œâ”€â”€ rollout_buffer.py        # Experience storage âœ…
â”‚   â”œâ”€â”€ curriculum.py            # Progressive learning âœ…
â”‚   â”œâ”€â”€ train.py                 # Training loop âœ…
â”‚   â””â”€â”€ test_result/             # Test reports âœ…
â”œâ”€â”€ data/
â”‚   â””â”€â”€ real_production_snapshot.json  # 295 jobs, 145 machines
â””â”€â”€ configs/
    â”œâ”€â”€ environment.yaml    # Game settings
    â”œâ”€â”€ training.yaml       # Hyperparameters
    â””â”€â”€ model.yaml          # Architecture config
```

## Curriculum Learning Stages

1. **Toy** (10 jobs, 5 machines) - Learn basic rules
2. **Small** (50 jobs, 20 machines) - Learn strategies
3. **Medium** (200 jobs, 50 machines) - Learn scaling
4. **Large** (500 jobs, 100 machines) - Near production
5. **Production** (295 jobs, 145 machines) - Full scale

## Next Steps

### Phase 3 - Training
1. Start curriculum learning from toy scale
2. Monitor training metrics
3. Adjust hyperparameters as needed
4. Progress through all 5 stages
5. Save best model checkpoints

### Phase 4 - Deployment
1. Build FastAPI inference server
2. Add working hours post-processing
3. Connect to frontend visualization
4. Compare with current scheduler
5. Deploy to production

## Success Metrics

### Achieved âœ…
- Handle variable job counts (10-1000+)
- Parse multi-machine requirements correctly
- Calculate processing times with capacity formula
- 100% test coverage and success rate
- Real production data integration

### Targets for Training
- Learn sequence constraints from experience
- Discover deadline prioritization
- Achieve 95%+ on-time delivery
- Minimize total makespan
- Zero constraint violations

## Key Insights

1. **Multi-Machine Jobs**: Some jobs require multiple machines working together simultaneously
2. **Processing Times**: Use capacity formula when applicable
3. **Working Hours**: Apply only during deployment, not training
4. **Pure Learning**: No hardcoded strategies - everything emerges from rewards
5. **Snapshot System**: 500x faster than live database queries

---

*Pure Deep Reinforcement Learning: Where scheduling strategies emerge from experience, not rules.*