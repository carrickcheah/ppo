# Deep Reinforcement Learning Scheduling System

## Project Status: Two Parallel Approaches

### 1. Production System (/app) - Phase 4 Deployed ‚úÖ
- **Achievement**: Full production scale with 152 machines
- **Performance**: 49.2h makespan with 100% completion rate
- **Status**: API deployed, Frontend enhanced, Database optimized
- **Latest**: Front2 integration showing all 100 scheduled tasks with real data

### 2. Pure DRL System (/app_2) - Phase 1 Complete üöÄ
- **Achievement**: Game-based environment with zero hardcoding
- **Approach**: AI learns all strategies through experience
- **Status**: Environment foundation complete, ready for PPO model
- **Philosophy**: User defines physics, AI discovers optimal play

## Latest Updates (July 23, 2025)

### Production System Enhancements
- **Database Optimization**: Created indexes reducing query time by 80-95% (5-15s ‚Üí 0.3-1s)
- **Frontend Enhancement**: Added tabbed navigation with separate Jobs and Machine charts
- **Front2 PPO Integration**: 
  - Dedicated frontend for PPO backend only
  - Fixed to display all 100 scheduled tasks (not just 27 merged jobs)
  - Shows real production data with full task IDs (e.g., `JOST25050298_CP01-123-1/3`)
- **Sequence Violation Root Cause**: Identified batch scheduler splitting job families

### Pure DRL Architecture (/app_2)
- **Environment Complete**: SchedulingGameEnv with Gymnasium interface
- **Hard Rules Engine**: Enforces only physics (sequence, compatibility, no overlap)
- **Reward Function**: Configurable signals for AI learning
- **Data Layer**: MariaDB integration with flexible data loading
- **Zero Hardcoding**: All parameters in YAML configurations

## System Architecture Comparison

### Production System (/app) - Traditional PPO
```
MariaDB ‚Üí Batch Scheduler ‚Üí PPO Model ‚Üí Schedule ‚Üí API
   ‚Üì           ‚Üì               ‚Üì           ‚Üì        ‚Üì
[Jobs]   [170 job batches] [Actions]  [Output] [Constraints]
         (Splits families!)
```

### Pure DRL System (/app_2) - Game-Based Learning
```
MariaDB ‚Üí Game Environment ‚Üí PPO Player ‚Üí Schedule ‚Üí API
   ‚Üì            ‚Üì                ‚Üì           ‚Üì        ‚Üì
[Jobs]    [State/Rules]    [Learn to Play] [Output] [Pure AI]
          (No batching!)
```

## Key Differences

| Aspect | Production System (/app) | Pure DRL System (/app_2) |
|--------|-------------------------|--------------------------|
| Approach | PPO with constraints | Pure learning from experience |
| Batching | 170 jobs/batch | No limits - handles all jobs |
| Strategies | Some hardcoded | Everything learned |
| Sequence | Can violate (batch splits) | Physics enforced |
| Action Space | Fixed size with padding | Variable with attention |
| Philosophy | Traditional RL | Game-based AI |

## Project Locations
- Production System: `/Users/carrickcheah/Project/ppo/app`
- Pure DRL System: `/Users/carrickcheah/Project/ppo/app_2`

## Quick Start

### Production System (/app)
```bash
cd /Users/carrickcheah/Project/ppo/app
source .venv/bin/activate
uv sync

# Check API status
uv run python src/deployment/api_server.py

# View frontend
cd ../frontend
npm start
```

### Pure DRL System (/app_2)
```bash
cd /Users/carrickcheah/Project/ppo/app_2
source .venv/bin/activate
uv sync

# Run environment tests
uv run python run_test.py

# Next: Implement PPO model (Phase 2)
```

## Development Status

### Production System - Complete Through Phase 5a
- ‚úÖ Phase 1-2: Toy environment (2 machines, 5 jobs)
- ‚úÖ Phase 3: Scaled production (40 machines, real data)
- ‚úÖ Phase 4: Full production (152 machines, API deployed)
- ‚úÖ Phase 5: Hierarchical action space research
- ‚úÖ Phase 5a: Frontend enhancement & database optimization

### Pure DRL System - Phase 1 Complete
- ‚úÖ Phase 1: Game environment foundation
  - Scheduling game with hard rules (physics)
  - Configurable rewards (learning signals)
  - MariaDB data integration
  - Comprehensive test suite
- üöß Phase 2: PPO model architecture (IN PROGRESS)
- ‚è≥ Phase 3: Training with curriculum learning
- ‚è≥ Phase 4: API deployment
- ‚è≥ Phase 5: Pure AI evolution

## Key Achievements

### Production System
- 49.2h makespan with 100% job completion
- API response time <100ms
- Database queries 80-95% faster with indexes
- Enhanced UI with job-centric and machine-centric views
- Real production data visualization

### Pure DRL System
- Zero hardcoded scheduling strategies
- Flexible to handle 10-1000+ jobs without modification
- Clean separation of game rules vs learned strategies
- All configuration externalized to YAML files
- Ready for transformer-based PPO implementation

## Next Steps

### Production System
1. Shadow mode testing with real production data
2. Gradual rollout (10% ‚Üí 50% ‚Üí 100%)
3. Continuous monitoring and improvement

### Pure DRL System
1. Implement transformer-based PPO model
2. Create action masking for valid moves only
3. Build training pipeline with curriculum learning
4. Train model to discover optimal strategies
5. Deploy and compare with production system

## Success Metrics

### Production System (Achieved)
- ‚úÖ Handle 411 jobs across 152 machines
- ‚úÖ 100% job completion rate
- ‚úÖ <1 second scheduling time
- ‚úÖ API and frontend deployed

### Pure DRL System (Targets)
- [ ] Handle 1000+ jobs without batching
- [ ] Learn sequence constraints without being told
- [ ] Discover priority patterns from rewards
- [ ] Achieve 95%+ on-time delivery
- [ ] Zero hardcoded scheduling rules

---

*Two approaches to scheduling: Traditional PPO with constraints (production ready) and Pure DRL where AI learns everything (research phase). The future is learning-based scheduling.*