### 2025-07-23 - Database Optimization & Frontend Enhancement
- **Database Performance Optimization**:
  - Identified slow "LOAD ALL JOBS FROM DATABASE" button performance (5-15 seconds)
  - Created `app/sql/create_indexes.sql` with comprehensive index optimization
  - Added composite indexes for `tbl_jo_txn` and `tbl_jo_process` tables
  - Implemented covering indexes to reduce I/O operations
  - Created `app/sql/README.md` with installation and maintenance instructions
  - Expected performance improvement: 80-95% faster (from 5-15s to 0.3-1s)
  - Successfully applied indexes to production database
- **Frontend UI Restructuring**:
  - Added tabbed navigation to separate Dashboard, Jobs Chart, and Machine Chart views
  - Implemented `ScheduleContext` for sharing schedule data between components
  - Created `JobsChart.tsx` component:
    - Job-centric Gantt view with families on Y-axis
    - Time range selector (7, 14, 30, 60 days)
    - Color-coded jobs (green: on-time, purple: medium priority, red: high priority)
    - Current time indicator and hover tooltips
  - Created `MachineChart.tsx` component:
    - Resource-centric view showing machine utilization
    - Machine names with utilization percentages
    - Similar time range and color coding
    - Machine utilization summary statistics
  - Modified `ScheduleForm.tsx`:
    - Removed default 20 jobs input field
    - Removed 'GENERATE SAMPLE' button
    - Removed 'LOAD FROM FILE' button
    - Added auto-load functionality on component mount
    - Changed to 'Refresh Jobs' button for manual reload
- **Key Improvements**:
  - Cleaner, more focused UI with dedicated views for different perspectives
  - Significantly faster job loading from database
  - Better user experience with automatic job loading
  - Maintained all existing functionality while improving usability

### 2025-07-23 - Front2 PPO Integration & Real Data Display
- **Front2 Configuration for PPO Backend**:
  - Created separate frontend instance (`front2`) dedicated to PPO backend only
  - Fixed import resolution error for `@/lib/utils` by creating missing utility file
  - Removed all constraint programming and fallback logic from DataCacheContext
  - Updated configuration to point all API calls to PPO backend (port 8000)
  - Fixed CORS configuration in PPO backend settings
- **Real Production Data Integration**:
  - Fixed DataCacheContext to use PPO schedule response directly
  - Removed jobDataService calls that tried to fetch from non-existent endpoints
  - PPO backend successfully schedules 100 tasks from 27 unique jobs
  - Verified all jobs use real production prefixes (JOST, JOTP, JOAW)
  - No synthetic or sample data - 100% real production data from MariaDB
- **Gantt Chart Task Display Fix**:
  - Fixed issue showing only 27 merged jobs instead of 100 scheduled tasks
  - Modified GanttChartDisplay to show individual tasks, not merged jobs
  - Updated ppoAdapter to generate unique task IDs with process notation
  - Task ID format: `JOBID_PROCESSCODE-INSTANCE/TOTAL` (e.g., `JOST25050298_CP01-123-1/3`)
  - All 100 tasks now display as separate rows on Y-axis
- **Key Achievements**:
  - Front2 successfully uses only PPO backend for scheduling
  - Real production data displayed correctly (100 tasks, 27 unique jobs)
  - Fixed working hours to use default values (PPO doesn't have this endpoint)
  - Gantt chart shows full task details as requested by user
  - System ready for production use with real data

### 2025-07-23 - Pure DRL Scheduling System Architecture & Implementation
- **Pure Deep Reinforcement Learning Design**:
  - Designed completely new architecture in `/app_2/` for pure DRL approach
  - No hardcoded rules or strategies - AI learns everything from experience
  - Game-based paradigm: User defines physics, AI learns to play optimally
  - Identified root cause of sequence violations: batch scheduler splitting job families
  - Decision to solve through pure learning instead of hardcoded fixes
- **Environment Implementation (Phase 1 Complete)**:
  - Created `SchedulingGameEnv` - Gymnasium environment for scheduling game
  - Implemented `RulesEngine` - Enforces only hard constraints (physics):
    - Sequence constraints within job families
    - Machine compatibility checking
    - No time overlap enforcement
    - Working hours constraints
  - Built `RewardFunction` - Provides learning signals:
    - Completion rewards, importance bonuses
    - Urgency multipliers based on deadlines
    - Efficiency and load balancing rewards
    - All configurable via YAML (no hardcoding)
  - Developed data layer:
    - `DBConnector` for MariaDB integration
    - `DataLoader` supporting database/snapshot/test sources
    - Clean data extraction without business logic
- **Configuration System**:
  - Created comprehensive YAML configurations:
    - `environment.yaml` - Game rules and reward settings
    - `training.yaml` - PPO hyperparameters and curriculum
    - `deployment.yaml` - Production API configuration
  - Zero hardcoding - all parameters externalized
- **Project Structure**:
  - Clean separation of concerns in `/app_2/`:
    - `/src/environment/` - Game mechanics
    - `/src/data/` - Data loading
    - `/src/model/` - (Phase 2) PPO networks
    - `/src/training/` - (Phase 2) Training pipeline
    - `/src/api/` - (Phase 4) Deployment
  - Created comprehensive test suite validating all components
- **Documentation Updates**:
  - Fixed encoding issues in z_TODO.md, z_FLOWS.md, README.md
  - Created detailed 5-phase implementation plan
  - Updated workflow documentation with pure DRL approach
  - Created SETUP.md with complete environment documentation
- **Key Design Principles**:
  - Variable job handling (10-1000+) using attention mechanisms
  - No batch limits or fixed strategies
  - AI discovers all patterns from rewards
  - Continuous learning and adaptation
  - Simple interface: Raw data in, optimized schedule out

### 2025-07-24 - Data Pipeline Fixes & Multi-Machine Understanding
- **Critical Schema Understanding**:
  - `Machine_v` contains machine IDs (not names or types)
  - Multiple IDs like "57,64,65,66,74" mean job requires ALL 5 machines simultaneously
  - This is NOT a choice - job occupies all listed machines at once
- **Processing Time Formula Implemented**:
  - When `CapMin_d = 1` and `CapQty_d > 0`: 
    - Hours = (JoQty_d / (CapQty_d * 60)) + (SetupTime_d / 60)
  - Verified with real data: 500 units at 6/min + 10min setup = 1.56 hours
  - CycleTime_d removed (always 0 in production)
- **Database Schema Updates**:
  - Added `IsImportant` column from tbl_jo_txn (boolean flag)
  - Added `CapQty_d`, `CapMin_d` for capacity calculations
  - Job ID now uses DocRef_v + Task_v for better identification
- **Key Findings from Testing**:
  - 81 pending jobs across 19 families
  - 5 multi-machine jobs found (requiring 2-5 machines simultaneously)
  - 12 jobs have no assigned machines (need handling)
  - Processing times range from 0.3 to 65 hours
- **Constraint Clarification**:
  - Hard: Sequence, machine requirements, no overlap
  - Soft: Deadlines, importance, efficiency, load balancing
  - Deployment: Working hours (not part of training)

### 2025-07-24 - Environment Updates for Multi-Machine Jobs
- **Multi-Machine Job Support Implemented**:
  - Updated `SchedulingGameEnv` to handle jobs requiring multiple machines simultaneously
  - Created `_schedule_multi_machine_job()` method that:
    - Finds earliest time ALL required machines are available
    - Schedules job on all machines for entire duration
    - Tracks multi-machine status in job assignments
  - Multi-machine jobs receive bonus reward for complexity
- **Action Masking Enhanced**:
  - Updated `get_action_mask()` to properly handle multi-machine jobs
  - Valid to select ANY required machine - environment schedules on ALL
  - Added `_check_multi_machine_availability()` helper method
- **Working Hours Removed from Training**:
  - Per user request, working hours NOT enforced during training
  - Updated `_adjust_for_working_hours()` to return time as-is
  - Changed config `enforce_working_hours: false`
  - Marked as deployment-only constraint in documentation
- **Testing Results**:
  - Successfully tested with synthetic multi-machine job (3 machines)
  - Verified all machines occupied for entire duration
  - Action masking correctly identifies valid machine choices
  - Production data test: 81 jobs, 145 machines working correctly
  - Sequence constraints properly enforced
  - Edge cases handled (invalid machines, out of bounds)

### 2025-07-23 - Database Integration & Production Data Testing
- **Database Schema Integration**:
  - Updated `DBConnector` to work with actual production schema
  - Jobs structured with `tbl_jo_txn` (transactions) and `tbl_jo_process` (process steps)
  - Each transaction (`TxnId_i`) contains multiple sequential processes (`RowId_i`)
  - Jobs grouped by `DocRef_v` (e.g., JOAW25060101) serving as family identifier
  - Machine compatibility stored in `Machine_v` field as comma-separated names
- **Production Data Characteristics**:
  - Successfully connected and tested with real data:
    - 233 pending jobs across 52 unique transactions
    - 145 active machines with 24+ different type IDs
    - Jobs maintain proper sequence structure (1/3, 2/3, 3/3)
  - Complex working hours discovered:
    - Different schedules per day (Mon-Thu: 18-19, Fri: 6-18, Sat: 6-13)
    - 8 different break periods including lunch, tea breaks, and machine downtime
  - Processing times appear to be in minutes (e.g., 61.0 hours may be 61 minutes)
- **Data Mapping Implementation**:
  - `DocRef_v` → family_id (groups related jobs)
  - `RowId_i` → sequence number within family
  - `Machine_v` → parsed to machine_types list
  - `DifficultyLevel_i >= 3` → is_important flag
  - `CycleTime_d + SetupTime_d` → total processing time
- **Key Findings**:
  - Job families properly maintain sequencing requirements
  - Machine type mapping successfully extracted from database
  - Working hours more complex than initially assumed

### 2025-07-24 - Phase 2 Complete: Pure DRL Implementation
- **Database Schema Corrections**:
  - Fixed critical misunderstanding: `Machine_v` contains machine IDs, not names
  - When `Machine_v = "57,64,65,66,74"`, job requires ALL 5 machines SIMULTANEOUSLY
  - Updated processing time calculation using capacity formula:
    - When `CapMin_d = 1` and `CapQty_d != 0`: `Hours = JoQty_d / (CapQty_d * 60)`
  - Removed working hours from training environment (deployment only)
- **Phase 1.5 - Data Pipeline Fixes**:
  - Updated `db_connector.py` with correct schema mapping
  - Fixed processing time calculation with capacity formula
  - Implemented multi-machine job parsing (comma-separated IDs)
  - Created production snapshot with 295 jobs from 88 families, 145 machines
- **Phase 1.6 - Environment Updates**:
  - Implemented `_schedule_multi_machine_job()` for simultaneous machine occupation
  - Updated action masking for multi-machine compatibility
  - Disabled working hours for training (moved to deployment phase)
  - Successfully tested with 5 multi-machine jobs in production data
- **Phase 2 - PPO Model Implementation**:
  - Built complete PPO architecture with transformer policy
  - Components created:
    - State encoder for variable-sized inputs (10-1000+ jobs)
    - Transformer policy with self-attention and cross-attention
    - Action masking module for valid action filtering
    - PPO scheduler with actor-critic architecture
    - Rollout buffer with GAE computation
    - Curriculum learning manager (toy → production scale)
    - Training script with progressive difficulty
    - Evaluation module for performance tracking
  - All files organized in `/app_2/phase2/` directory
- **Comprehensive Testing**:
  - Created test suite covering all components
  - Initial test results: 64.3% success rate (9/14 passed)
  - Fixed issues:
    - Data loader now handles families structure from snapshot
    - Rules engine test updated with correct method signature
    - Reward function test uses `calculate_step_reward` method
    - Transformer policy test fixed with correct embed_dim (256)
    - Integration test fixed with proper machine ID mapping
  - Final test results: **100% success rate (14/14 passed)**
  - Test report saved to `/app_2/phase2/test_result/comprehensive_report.md`
- **Data Documentation**:
  - Created detailed data usage report showing:
    - 295 real production tasks from 88 families
    - 145 real machines from MariaDB
    - 5 multi-machine jobs (2-5 machines each)
    - Processing time formula and transformations
  - Created beginner-friendly snapshot explanation
  - Snapshot benefits: 500x faster, consistent, offline-capable
- **System Status**: Ready for Phase 3 (Training)
  - All components tested and working
  - Real production data loaded and validated
  - PPO model initialized and ready
  - Curriculum learning configured (5 stages)
  - Ready for environment testing with real production constraints