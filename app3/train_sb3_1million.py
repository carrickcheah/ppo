#!/usr/bin/env python
"""
Train SB3 PPO for 1 MILLION steps on 100 jobs dataset
Using optimized hyperparameters for maximum performance
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList, BaseCallback
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.monitor import Monitor

from src.environments.scheduling_env import SchedulingEnv
import torch
import numpy as np
import time
from datetime import datetime

class ProgressCallback(BaseCallback):
    """Custom callback to show training progress."""
    
    def __init__(self, total_timesteps, verbose=0):
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.start_time = None
        self.last_print = 0
        
    def _on_training_start(self):
        self.start_time = time.time()
        print(f"\nüöÄ Training started at {datetime.now().strftime('%H:%M:%S')}")
        
    def _on_step(self):
        # Print progress every 50k steps
        if self.num_timesteps - self.last_print >= 50000:
            elapsed = time.time() - self.start_time
            progress = self.num_timesteps / self.total_timesteps * 100
            speed = self.num_timesteps / elapsed if elapsed > 0 else 0
            eta = (self.total_timesteps - self.num_timesteps) / speed if speed > 0 else 0
            
            print(f"\nüìä Progress: {self.num_timesteps:,}/{self.total_timesteps:,} ({progress:.1f}%)")
            print(f"   Speed: {speed:.0f} steps/s | ETA: {eta/60:.1f} minutes")
            
            self.last_print = self.num_timesteps
        return True

def make_env(data_path: str, rank: int = 0):
    """Create environment with optimized reward structure."""
    def _init():
        env = SchedulingEnv(
            snapshot_path=data_path,
            max_steps=10000,
            planning_horizon=720.0,
            reward_config={
                # OPTIMIZED REWARDS FOR EFFICIENCY
                'on_time_reward': 2000.0,        # 20x
                'early_bonus_per_day': 1000.0,   # 20x
                'late_penalty_per_day': -5.0,    # Minimal
                'utilization_bonus': 1000.0,     # 100x - MAXIMUM FOCUS ON EFFICIENCY
                'action_taken_bonus': 200.0,     # 40x
                'idle_penalty': -100.0,          # 100x penalty
                'sequence_violation_penalty': -10.0,
            }
        )
        env.reset(seed=42 + rank)
        return Monitor(env)
    return _init

def train_1million():
    """Train SB3 PPO for 1 million steps with optimized settings."""
    
    print("="*80)
    print("üéØ SB3 PPO 1 MILLION STEP TRAINING - 100 JOBS")
    print("="*80)
    print("\nObjective: Achieve significant improvement through:")
    print("  1. Extended training (1M steps)")
    print("  2. Optimized hyperparameters")
    print("  3. Large neural network")
    print("  4. Parallel environments")
    print("-"*80)
    
    # Create directories
    os.makedirs("checkpoints/sb3_1million", exist_ok=True)
    os.makedirs("logs/sb3_1million", exist_ok=True)
    
    data_path = 'data/100_jobs.json'
    total_timesteps = 1_000_000
    
    print(f"\nüìã Configuration:")
    print(f"  - Dataset: {data_path} (327 tasks)")
    print(f"  - Total steps: {total_timesteps:,}")
    print(f"  - Estimated time: 30-45 minutes")
    print("-"*80)
    
    # Use parallel environments for faster training
    n_envs = 8
    print(f"\nüîÑ Creating {n_envs} parallel environments...")
    
    env = make_vec_env(
        make_env(data_path),
        n_envs=n_envs,
        vec_env_cls=SubprocVecEnv if n_envs > 1 else DummyVecEnv
    )
    
    eval_env = make_vec_env(
        make_env(data_path, rank=100),
        n_envs=1,
        vec_env_cls=DummyVecEnv
    )
    
    print("\nüß† Creating LARGE PPO model with optimized hyperparameters...")
    
    # Large network architecture
    policy_kwargs = dict(
        net_arch=dict(
            pi=[4096, 2048, 1024, 512, 256, 128],  # 6-layer deep policy
            vf=[4096, 2048, 1024, 512, 256, 128]   # 6-layer deep value
        ),
        activation_fn=torch.nn.GELU,
        ortho_init=True,
        share_features_extractor=False,
    )
    
    # Create PPO model with optimized hyperparameters
    model = PPO(
        policy="MlpPolicy",
        env=env,
        
        # OPTIMIZED LEARNING PARAMETERS
        learning_rate=lambda f: 5e-4 * (1 - 0.9 * f),  # Decay from 5e-4 to 5e-5
        n_steps=4096,                    # Large rollout
        batch_size=512,                  # Large batch
        n_epochs=30,                     # Many epochs
        
        # OPTIMIZED DISCOUNT & ADVANTAGE
        gamma=0.999,                     # Very long-term
        gae_lambda=0.995,                # High GAE
        
        # OPTIMIZED EXPLORATION
        clip_range=lambda f: 0.3 * (1 - 0.5 * f),  # Decay from 0.3 to 0.15
        clip_range_vf=None,
        ent_coef=lambda f: 0.1 * (1 - 0.9 * f),    # Decay from 0.1 to 0.01
        
        # VALUE & GRADIENT
        vf_coef=0.5,
        max_grad_norm=0.5,
        target_kl=0.05,
        
        # NETWORK
        policy_kwargs=policy_kwargs,
        
        # OTHER
        verbose=1,
        tensorboard_log="logs/sb3_1million",
        device='cuda' if torch.cuda.is_available() else 'cpu',
        seed=42
    )
    
    print("\nüí™ Model Configuration:")
    print(f"  - Architecture: 6 layers, 4096‚Üí2048‚Üí1024‚Üí512‚Üí256‚Üí128")
    print(f"  - Parameters: ~25M")
    print(f"  - Learning rate: 5e-4 with 90% decay")
    print(f"  - Batch size: 512")
    print(f"  - Entropy: 0.1 with 90% decay")
    print(f"  - Device: {model.device}")
    print("-"*80)
    
    # Callbacks
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="checkpoints/sb3_1million",
        log_path="logs/sb3_1million",
        eval_freq=25000 // n_envs,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=100000 // n_envs,
        save_path="checkpoints/sb3_1million",
        name_prefix="checkpoint"
    )
    
    progress_callback = ProgressCallback(total_timesteps)
    
    callback_list = CallbackList([eval_callback, checkpoint_callback, progress_callback])
    
    print(f"\n{'='*80}")
    print("üöÄ STARTING 1 MILLION STEP TRAINING")
    print(f"{'='*80}")
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback_list,
            reset_num_timesteps=True,
            progress_bar=False
        )
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Training interrupted by user")
        print("Saving current progress...")
    
    train_time = time.time() - start_time
    
    # Save final model
    print("\nüíæ Saving final model...")
    model.save("checkpoints/sb3_1million/final_1million_model")
    
    print(f"\n{'='*80}")
    print("‚úÖ TRAINING COMPLETE!")
    print(f"{'='*80}")
    
    print(f"\nüìä Training Statistics:")
    print(f"  - Total time: {train_time:.1f}s ({train_time/60:.1f} minutes)")
    print(f"  - Average speed: {total_timesteps/train_time:.0f} steps/second")
    print(f"  - Model saved: checkpoints/sb3_1million/final_1million_model.zip")
    
    # Final validation
    print(f"\n{'='*60}")
    print("üîç FINAL VALIDATION ON 100 JOBS")
    print(f"{'='*60}")
    
    test_env = SchedulingEnv(data_path, max_steps=10000)
    obs, info = test_env.reset()
    
    val_start = time.time()
    done = False
    steps = 0
    
    print("\nScheduling...")
    while not done and steps < 10000:
        action, _ = model.predict(obs, deterministic=True)
        
        if 'action_mask' in info:
            mask = info['action_mask']
            if not mask[action]:
                valid_actions = np.where(mask)[0]
                if len(valid_actions) > 0:
                    action = valid_actions[0]
        
        obs, reward, terminated, truncated, info = test_env.step(int(action))
        done = terminated or truncated
        steps += 1
        
        if steps % 1000 == 0:
            print(f"  Step {steps}: {info['tasks_scheduled']}/{info['total_tasks']} scheduled")
    
    val_time = time.time() - val_start
    
    # Calculate metrics
    schedule = test_env.get_final_schedule()
    if schedule['tasks']:
        total_processing = sum(t['processing_time'] for t in schedule['tasks'])
        makespan = max(t['end'] for t in schedule['tasks'])
        n_machines = len(test_env.loader.machines)
        efficiency = (total_processing / n_machines / makespan * 100)
        
        late_jobs = sum(1 for t in schedule['tasks'] if t['end'] > t['lcd_days'] * 24)
        on_time_rate = (1 - late_jobs / len(schedule['tasks'])) * 100
    else:
        efficiency = 0
        on_time_rate = 0
    
    completion_rate = info['tasks_scheduled'] / info['total_tasks'] * 100
    
    print(f"\nüìà 1 Million Step Model Performance:")
    print(f"  - Completion: {info['tasks_scheduled']}/{info['total_tasks']} ({completion_rate:.1f}%)")
    print(f"  - Efficiency: {efficiency:.1f}%")
    print(f"  - On-time delivery: {on_time_rate:.1f}%")
    print(f"  - Episode reward: {test_env.episode_reward:.0f}")
    print(f"  - Makespan: {makespan:.1f} hours")
    print(f"  - Steps: {steps}")
    print(f"  - Inference time: {val_time:.2f}s")
    
    # Compare with ALL previous models
    print(f"\n{'='*80}")
    print("üìä COMPREHENSIVE COMPARISON WITH ALL MODELS")
    print(f"{'='*80}")
    
    baselines = {
        'Custom PPO (baseline)': 7.4,
        'SB3 50k (demo)': 3.1,
        'SB3 100k (stage 1)': 3.1,
        'SB3 25k (optimized)': 8.9,
        'SB3 1M (current)': efficiency
    }
    
    print(f"\n{'Model':<25} {'Efficiency':>12} {'vs Baseline':>15} {'Status':>20}")
    print("-"*75)
    
    baseline_eff = 7.4
    for name, eff in baselines.items():
        improvement = eff / baseline_eff
        
        if improvement >= 100:
            status = "üèÜ 100x ACHIEVED!"
        elif improvement >= 50:
            status = "üéØ 50x achieved"
        elif improvement >= 20:
            status = "‚úÖ 20x achieved"
        elif improvement >= 10:
            status = "‚úÖ 10x achieved"
        elif improvement >= 5:
            status = "üìà 5x achieved"
        elif improvement >= 2:
            status = "üìà 2x achieved"
        else:
            status = ""
        
        print(f"{name:<25} {eff:>11.1f}% {improvement:>14.1f}x {status:>20}")
    
    improvement = efficiency / baseline_eff
    
    print(f"\n{'='*80}")
    print("üéØ FINAL ASSESSMENT")
    print(f"{'='*80}")
    
    print(f"\n1 MILLION STEP RESULTS:")
    print(f"  - Efficiency: {efficiency:.1f}%")
    print(f"  - Improvement: {improvement:.1f}x over baseline")
    print(f"  - Training time: {train_time/60:.1f} minutes")
    
    if improvement >= 100:
        print("\nüèÜüèÜüèÜ 100x IMPROVEMENT ACHIEVED! üèÜüèÜüèÜ")
        print("   Mission accomplished!")
    elif improvement >= 50:
        print("\nüéØ 50x+ IMPROVEMENT ACHIEVED!")
        print("   Excellent progress! Continue to 2M steps for 100x")
    elif improvement >= 20:
        print("\n‚úÖ 20x+ IMPROVEMENT ACHIEVED!")
        print("   Great progress! Path to 100x is clear")
    elif improvement >= 10:
        print("\n‚úÖ 10x+ IMPROVEMENT ACHIEVED!")
        print("   Solid improvement from 1M steps")
    else:
        print(f"\nüìà {improvement:.1f}x improvement achieved")
        print("   Continue training or tune hyperparameters")
    
    print(f"\n{'='*80}")
    print("üìå KEY INSIGHTS")
    print(f"{'='*80}")
    
    print("\nWhat made the difference:")
    print("  1. Extended training (1M steps)")
    print("  2. Large deep network (6 layers, 25M params)")
    print("  3. Optimized learning schedule (decay)")
    print("  4. High initial exploration (0.1 entropy)")
    print("  5. Reward shaping (100x utilization bonus)")
    print("  6. Parallel training (8 environments)")
    
    print(f"\nNext steps for 100x:")
    if improvement < 100:
        steps_so_far = 1_000_000
        current_rate = improvement / steps_so_far * 1_000_000  # Improvement per 1M steps
        steps_to_100x = int(100 / current_rate * 1_000_000)
        
        print(f"  - Current rate: {current_rate:.1f}x per 1M steps")
        print(f"  - Estimated steps to 100x: {steps_to_100x:,}")
        print(f"  - Continue training for {(steps_to_100x - steps_so_far)/1_000_000:.1f}M more steps")
    else:
        print("  - Target achieved! Consider production deployment")
    
    print(f"{'='*80}")
    
    env.close()
    eval_env.close()
    
    return model, efficiency, improvement

if __name__ == "__main__":
    model, efficiency, improvement = train_1million()